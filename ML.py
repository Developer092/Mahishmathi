# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_Fi-XnpmpM1yu0X0rj9TC29BkGCDxeiW
"""

#1. Mean ,Median , Mode


import statistics
import numpy as np

def calculate_statistics(data):
    try:
        results = {
            "Mean": statistics.mean(data),
            "Median": statistics.median(data),
            "Mode": statistics.mode(data) if len(set(data)) != len(data) else "No mode",
            "Variance": statistics.pvariance(data),  # population variance
            "Standard Deviation": statistics.pstdev(data)  # population standard deviation
        }
        return results
    except statistics.StatisticsError as e:
        return {"Error": str(e)}
    except ZeroDivisionError:
        return {"Error": "Empty dataset provided"}

# Example usage
if __name__ == "__main__":
    data = [2, 4, 4, 4, 5, 5, 7, 9]
    results = calculate_statistics(data)

    print("Statistical Measures:")
    for key, value in results.items():
        print(f"{key}:¬†{value}")

#4. simple linear regression


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Sample dataset (can be replaced by user input or CSV)
data = {
    'Hours_Studied': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'Marks_Obtained': [35, 40, 50, 55, 60, 65, 70, 75, 80, 85]
}
df = pd.DataFrame(data)

# Features and Target
X = df[['Hours_Studied']]  # independent variable
y = df['Marks_Obtained']   # dependent variable

# Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Model Training
model = LinearRegression()
model.fit(X_train, y_train)

# Prediction
y_pred = model.predict(X_test)

# Evaluation
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Outputs
print("‚úÖ Coefficient (Slope):", model.coef_[0])
print("‚úÖ Intercept:", model.intercept_)
print("‚úÖ Mean Squared Error:", mse)
print("‚úÖ R2 Score:", r2)

# Visualization
plt.scatter(X, y, color='red', label="Actual Marks")
plt.plot(X, model.predict(X), color='blue', label="Regression Line")
plt.xlabel('Hours Studied')
plt.ylabel('Marks Obtained')
plt.title('Simple Linear Regression')
plt.legend()
plt.grid(True)
plt.show()

#5. Multiple Linear regression (House Pricing)


import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Sample house dataset
data = {
    'Area': [1500, 1800, 2400, 3000, 3500],
    'Bedrooms': [3, 4, 3, 5, 4],
    'Age': [10, 15, 20, 18, 8],
    'Price': [400000, 500000, 600000, 650000, 620000]
}

df = pd.DataFrame(data)

# Features and Target
X = df[['Area', 'Bedrooms', 'Age']]
y = df['Price']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model training
model = LinearRegression()
model.fit(X_train, y_train)

# Prediction
y_pred = model.predict(X_test)

# Evaluation
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Outputs
print("‚úÖ Coefficients:", model.coef_)
print("‚úÖ Intercept:", model.intercept_)
print("‚úÖ Mean Squared Error:", mse)
print("‚úÖ R2 Score:", r2)
import matplotlib.pyplot as plt

# Plot Actual vs Predicted

plt.plot(y_test.values, label="Actual Price", marker='o')
plt.plot(y_pred, label="Predicted Price", marker='x')
plt.title("üè† House Price Prediction: Actual vs Predicted")
plt.xlabel("Test Sample Index")
plt.ylabel("Price")
plt.legend()
plt.grid(True)
plt.show()

#6. Decision tree using sklearn



from sklearn.datasets import load_breast_cancer
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt

# Load dataset
cancer = load_breast_cancer()
X = cancer.data
y = cancer.target

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

# Train model
clf = DecisionTreeClassifier(max_depth=3, criterion='entropy')
clf.fit(X_train, y_train)

# Predict
y_pred = clf.predict(X_test)

# Evaluate
print("‚úÖ Accuracy:", accuracy_score(y_test, y_pred))
print("‚úÖ Classification Report:\n", classification_report(y_test, y_pred))

# Visualize tree
plt.figure(figsize=(16, 10))  # Made larger to accommodate more features
plot_tree(clf, feature_names=cancer.feature_names, class_names=cancer.target_names, filled=True)
plt.title("Decision Tree - Breast Cancer Dataset")
plt.show()

#7. KNN using sklearn


from sklearn.datasets import load_digits
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix

# Load data
digits = load_digits()
X = digits.data
y = digits.target

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)

# Model
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)

# Predict
y_pred = knn.predict(X_test)

# Evaluate
print("‚úÖ Accuracy:", accuracy_score(y_test, y_pred))
print("‚úÖ Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

#8. Logistic regression using sklearn


from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Load dataset
data = load_breast_cancer()
X = data.data
y = data.target

# Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# Model
model = LogisticRegression(max_iter=5000)
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluate
print("‚úÖ Accuracy:", accuracy_score(y_test, y_pred))
print("‚úÖ Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("‚úÖ Classification Report:\n", classification_report(y_test, y_pred))

#9. K-means clustering



from sklearn.cluster import KMeans
from sklearn.datasets import load_breast_cancer
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load data
cancer = load_breast_cancer()
X = cancer.data
df = pd.DataFrame(X, columns=cancer.feature_names)

# Model - using 2 clusters for benign/malignant
kmeans = KMeans(n_clusters=2, random_state=0, n_init='auto')
df['Cluster'] = kmeans.fit_predict(X)

# Visualize using two key features
plt.figure(figsize=(10, 6))  # Slightly larger for better readability
sns.scatterplot(data=df,
                x='mean radius',
                y='mean texture',
                hue='Cluster',
                palette='Set1',
                s=70)  # Increased point size
plt.title('K-Means Clustering on Breast Cancer Dataset')
plt.xlabel('Mean Radius (tumor size)')
plt.ylabel('Mean Texture (cell consistency)')
plt.grid(True)
plt.show()